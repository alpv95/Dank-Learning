{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some basic imports and setups\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#mean of imagenet dataset in BGR\n",
    "imagenet_mean = np.array([104., 117., 124.], dtype=np.float32)\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "image_dir = os.path.join(current_dir, 'memes')\n",
    "#image_dir = current_dir\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alexnet import AlexNet\n",
    "\n",
    "#placeholder for input and dropout rate\n",
    "x = tf.placeholder(tf.float32, [1, 227, 227, 3])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "#create model with default config ( == no skip_layer and 1000 units in the last layer)\n",
    "model = AlexNet(x, keep_prob, 1000,[],['fc7','fc8'],512,weights_path='/data/alpv95/MemeProject/im2txt/bvlc_alexnet.npy') #maybe need to put fc8 in skip_layers\n",
    "\n",
    "#define activation of last layer as score\n",
    "score = model.fc6\n",
    "\n",
    "#create op to calculate softmax \n",
    "#softmax = tf.nn.softmax(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Converting captions and meme vector representations into single Tfrecord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requires putting memes through alexnet to find their vector rep, shuffling the captions, changing captions into their word2idx, finally saving one caption together with one meme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/data/alpv95/MemeProject/im2txt/memes/y-u-no.jpg', '/data/alpv95/MemeProject/im2txt/memes/mendozameme.jpg')\n",
      "(0, 194, 194)\n",
      "(100, 16802, 16802)\n",
      "sizing error\n",
      "(200, 35976, 35976)\n",
      "sizing error\n",
      "sizing error\n",
      "sizing error\n",
      "(300, 55027, 55027)\n",
      "sizing error\n",
      "sizing error\n",
      "(400, 74039, 74039)\n",
      "sizing error\n",
      "sizing error\n",
      "(500, 92598, 92598)\n",
      "sizing error\n",
      "sizing error\n",
      "(600, 111071, 111071)\n",
      "sizing error\n",
      "(700, 129455, 129455)\n",
      "sizing error\n",
      "(800, 147576, 147576)\n",
      "sizing error\n",
      "sizing error\n",
      "sizing error\n",
      "(900, 165608, 165608)\n",
      "(1000, 183722, 183722)\n",
      "sizing error\n",
      "(1100, 201838, 201838)\n",
      "(1200, 218644, 218644)\n",
      "sizing error\n",
      "(1300, 236360, 236360)\n",
      "sizing error\n",
      "sizing error\n",
      "(1400, 252935, 252935)\n",
      "sizing error\n",
      "sizing error\n",
      "sizing error\n",
      "(1500, 269052, 269052)\n",
      "(1600, 285491, 285491)\n",
      "sizing error\n",
      "sizing error\n",
      "(1700, 301117, 301117)\n",
      "sizing error\n",
      "(1800, 315906, 315906)\n",
      "sizing error\n",
      "(1900, 330982, 330982)\n",
      "(2000, 346091, 346091)\n",
      "sizing error\n",
      "sizing error\n",
      "(2100, 361910, 361910)\n",
      "(2200, 375182, 375182)\n",
      "sizing error\n",
      "sizing error\n",
      "(2300, 389491, 389491)\n",
      "sizing error\n",
      "sizing error\n",
      "(2400, 402469, 402469)\n",
      "(2500, 414398, 414398)\n"
     ]
    }
   ],
   "source": [
    "with open('/data/alpv95/MemeProject/im2txt/ordered_memes.txt','r') as f:\n",
    "    img_files = f.readlines()\n",
    "img_files = [os.path.join(image_dir, f) for f in img_files] # add path to each file\n",
    "img_files = [img_file.replace('\\n','') for img_file in img_files]\n",
    "print(img_files[0],img_files[-1])\n",
    "with open('/data/alpv95/MemeProject/im2txt/Captions.txt','r') as f:\n",
    "    captions = f.readlines()\n",
    "#captions = list(set(captions))\n",
    "captions = [s.lower() for s in captions]\n",
    "deleters = []\n",
    "for i,capt in enumerate(captions):\n",
    "    if ' - ' not in capt or ' - -' in capt:\n",
    "        deleters.append(i)\n",
    "for i,delete in enumerate(deleters):\n",
    "    del captions[delete-i]\n",
    "data_memes = []\n",
    "data_captions = []\n",
    "counter = 0\n",
    "\n",
    "#Doing everything in one script: (the fc6 vectors are quite sparse)\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Load the pretrained weights into the model\n",
    "    model.load_initial_weights(sess)\n",
    "    \n",
    "    for i,meme in enumerate(img_files):\n",
    "        #meme_name = meme.replace('/Users/ALP/Desktop/Stanford/CS224n/MemeProject/memes/','')\n",
    "        #meme_name = meme_name.replace('.jpg','').lower()\n",
    "        #meme_name = meme_name.replace('-',' ')\n",
    "        img = Image.open(meme)\n",
    "        try:\n",
    "            img.thumbnail((227, 227), Image.ANTIALIAS)\n",
    "            #img = img.resize((227,227))\n",
    "            #use img.thumbnail for square images, img.resize for non square\n",
    "            assert np.shape(img) == (227, 227, 3)\n",
    "        except AssertionError:\n",
    "            img = img.resize((227,227))\n",
    "            print('sizing error')\n",
    "        \n",
    "        # Subtract the ImageNet mean\n",
    "        img = img - imagenet_mean #should probably change this\n",
    "        \n",
    "        # Reshape as needed to feed into model\n",
    "        img = img.reshape((1,227,227,3))\n",
    "\n",
    "        meme_vector = sess.run(score, feed_dict={x: img, keep_prob: 1}) #[1,4096]\n",
    "        meme_vector = np.reshape(meme_vector,[4096])\n",
    "        assert np.shape(meme_vector) == (4096,)\n",
    "        #match = [s.split('-',1)[-1].lstrip() for s in captions if meme_name in s]\n",
    "        \n",
    "        match = []\n",
    "        meme_name = captions[counter].split(' - ')[0]\n",
    "        \n",
    "        while meme_name in captions[counter]:\n",
    "                if counter==len(captions)-1:\n",
    "                    match.append(captions[counter].split(' - ')[-1])\n",
    "                    break\n",
    "                elif captions[counter] == captions[counter].split(' - ')[-1]:\n",
    "                    counter += 1\n",
    "                else:\n",
    "                    match.append(captions[counter].split(' - ')[-1])\n",
    "                    counter += 1\n",
    "                \n",
    "        \n",
    "        #now save in tfrecords format, or prepare for that action\n",
    "        meme_vectors = [meme_vector for cap in match]\n",
    "        assert len(meme_vectors) == len(match)\n",
    "        data_memes.extend(meme_vectors)\n",
    "        data_captions.extend(match)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(i,len(data_memes),len(data_captions))\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414401\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "385708"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = list(zip(data_memes, data_captions))\n",
    "no_repeats = []\n",
    "# order preserving\n",
    "def idfun(x): return x\n",
    "\n",
    "seen = {}\n",
    "no_repeats = []\n",
    "for item in c:\n",
    "    marker = idfun(item[1])\n",
    "    # in old Python versions:\n",
    "    # if seen.has_key(marker)\n",
    "    # but in new ones:\n",
    "    if marker in seen: continue\n",
    "    seen[marker] = 1\n",
    "    no_repeats.append(item)\n",
    "print(len(data_captions))\n",
    "len(no_repeats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "shuffle(no_repeats)\n",
    "memes_shuffled, captions_shuffled = zip(*no_repeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scored 4 times quick wasnt quick enough\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_shuffled[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385708\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "word_captions = []\n",
    "for capt in captions_shuffled:\n",
    "    words = re.findall(r\"[\\w']+|[.,!?;'><(){}%$#£@-_+=|\\/~`^&*]\", capt)\n",
    "    word_captions.append(words)\n",
    "print(len(word_captions))\n",
    "#word_captions = list(set(word_captions))\n",
    "#print(len(word_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocabulary.\n",
      "('Total words:', 150280)\n",
      "('Words in vocabulary:', 41153)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(\"Creating vocabulary.\")\n",
    "counter = Counter()\n",
    "for c in word_captions:\n",
    "    counter.update(c)\n",
    "print(\"Total words:\", len(counter))\n",
    "\n",
    "# Filter uncommon words and sort by descending count.\n",
    "word_counts = [x for x in counter.items() if x[1] >= 3]\n",
    "word_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"Words in vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vocabulary dictionary.\n",
    "reverse_vocab = [x[0] for x in word_counts]\n",
    "#unk_id = len(reverse_vocab)\n",
    "vocab_dict = dict([(x, y) for (y, x) in enumerate(reverse_vocab)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict['hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "38520\n",
      "('Wrote vocabulary file:', 'vocab3.txt')\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIMENSION=300 # Available dimensions for 6B data is 50, 100, 200, 300\n",
    "data_directory = current_dir\n",
    "\n",
    "PAD_TOKEN = 0\n",
    "\n",
    "word2idx = { 'PAD': PAD_TOKEN } # dict so we can lookup indices for tokenising our text later from string to sequence of integers\n",
    "weights = []\n",
    "index_counter = 0\n",
    "\n",
    "with open('glove.42B.300d.txt','r') as file:\n",
    "    for index, line in enumerate(file):\n",
    "        values = line.split() # Word and weights separated by space\n",
    "        word = values[0] # Word is first symbol on each line\n",
    "        if word in vocab_dict:\n",
    "            index_counter += 1\n",
    "            word_weights = np.asarray(values[1:], dtype=np.float32) # Remainder of line is weights for word\n",
    "            word2idx[word] = index_counter # PAD is our zeroth index so shift by one\n",
    "            weights.append(word_weights)\n",
    "        if index % 100000 == 0:\n",
    "            print(index)\n",
    "        if index + 1 == 2000000:\n",
    "            # Limit vocabulary to top 40k terms\n",
    "            break\n",
    "\n",
    "EMBEDDING_DIMENSION = len(weights[0])\n",
    "# Insert the PAD weights at index 0 now we know the embedding dimension\n",
    "weights.insert(0, np.random.randn(EMBEDDING_DIMENSION))\n",
    "\n",
    "# Append unknown and pad to end of vocab and initialize as random #maybe include start and end token here\n",
    "UNKNOWN_TOKEN=len(weights)\n",
    "word2idx['UNK'] = UNKNOWN_TOKEN\n",
    "word2idx['<S>'] = UNKNOWN_TOKEN + 1\n",
    "word2idx['</S>'] = UNKNOWN_TOKEN + 2\n",
    "weights.append(np.random.randn(EMBEDDING_DIMENSION)*0.5)\n",
    "weights.append(np.random.randn(EMBEDDING_DIMENSION)*0.5)\n",
    "weights.append(np.random.randn(EMBEDDING_DIMENSION)*0.5)\n",
    "\n",
    "# Construct our final vocab\n",
    "weights = np.asarray(weights, dtype=np.float32)\n",
    "\n",
    "VOCAB_SIZE=weights.shape[0]\n",
    "print(VOCAB_SIZE)\n",
    "\n",
    "#Save Vocabulary\n",
    "with tf.gfile.FastGFile('vocab3.txt', \"w\") as f:\n",
    "    f.write(\"\\n\".join([\"%s %d\" % (w, c) for w, c in word2idx.iteritems()]))\n",
    "print(\"Wrote vocabulary file:\", 'vocab3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.44847  , -0.039212 ,  0.10393  , -0.18311  , -0.22794  ,\n",
       "        0.071043 ,  0.68074  , -1.0702   ,  0.10879  ,  0.32581  ,\n",
       "       -0.0075187, -0.7147   ,  0.38525  , -0.035042 ,  0.084247 ,\n",
       "        0.21304  ,  0.025578 , -0.38933  , -0.43553  , -0.17755  ,\n",
       "        0.4037   , -0.10913  , -0.056811 , -0.044236 , -0.12161  ,\n",
       "       -0.37139  , -0.3668   , -0.78518  ,  0.1406   ,  0.41383  ,\n",
       "       -0.06213  ,  0.64063  , -0.43111  , -0.16398  ,  0.15145  ,\n",
       "        0.51573  ,  0.39917  , -0.4754   ,  0.24798  , -0.13286  ,\n",
       "        0.30063  , -0.077097 , -0.064587 ,  0.30917  ,  0.65839  ,\n",
       "        0.0059184,  0.10049  ,  0.34199  ,  0.16988  ,  0.15465  ,\n",
       "        0.2108   , -0.16124  ,  0.16009  , -0.058738 ,  0.0602   ,\n",
       "       -0.4676   , -0.41398  ,  0.13282  ,  0.34347  ,  0.31409  ,\n",
       "       -0.2003   , -0.18261  ,  0.28005  ,  0.37784  , -0.085211 ,\n",
       "       -0.82782  , -0.19752  , -0.16085  , -0.051853 , -0.046978 ,\n",
       "       -0.090662 , -0.50244  ,  0.24386  ,  0.29688  , -0.2001   ,\n",
       "        0.19445  , -0.28373  , -0.18313  , -0.39333  ,  0.065854 ,\n",
       "       -0.011839 , -0.23055  , -0.43664  ,  0.080693 , -0.26789  ,\n",
       "       -0.3679   , -0.35613  ,  0.056707 , -0.20576  ,  0.17426  ,\n",
       "       -0.22824  ,  0.15239  , -0.056002 , -0.0097033, -0.30139  ,\n",
       "        0.14433  ,  0.40497  ,  0.054102 ,  0.68279  ,  0.14556  ,\n",
       "        0.0012223,  0.35829  , -0.1578   , -0.26101  , -0.19565  ,\n",
       "        0.10731  ,  0.049116 , -0.25991  ,  0.20451  ,  0.37388  ,\n",
       "        0.091763 ,  0.01326  , -0.16377  , -0.058506 , -0.21469  ,\n",
       "        0.060703 ,  0.16069  , -0.59625  , -0.039471 , -0.17519  ,\n",
       "        0.17627  , -0.35163  , -0.13807  ,  0.18959  , -0.33912  ,\n",
       "       -0.017737 ,  0.2403   ,  0.48794  ,  0.090305 ,  0.24129  ,\n",
       "        0.1004   ,  0.14941  , -0.040279 , -0.23966  , -0.010672 ,\n",
       "       -0.14114  ,  0.25642  , -0.29952  , -0.15572  , -0.66248  ,\n",
       "        0.17192  , -0.20204  ,  0.24225  ,  0.05396  ,  0.08926  ,\n",
       "        0.12152  , -0.3615   ,  0.15428  ,  0.27732  , -0.089112 ,\n",
       "        0.31128  , -0.14316  ,  0.18377  , -0.10577  ,  0.5478   ,\n",
       "        0.12911  ,  0.47722  , -0.50255  ,  0.12407  , -0.060889 ,\n",
       "        0.11583  ,  0.22377  , -0.39128  , -0.097678 , -0.16674  ,\n",
       "       -0.50881  ,  0.27624  , -0.39481  ,  0.17492  ,  0.63578  ,\n",
       "       -0.15305  ,  0.16731  , -0.030143 ,  0.35387  ,  0.18817  ,\n",
       "       -0.52209  , -0.13349  , -0.2518   , -0.51798  ,  0.62682  ,\n",
       "        0.060017 , -0.041401 , -0.028021 ,  0.11233  ,  0.11447  ,\n",
       "        0.44617  , -0.032094 , -0.052696 , -0.038342 ,  0.3574   ,\n",
       "        0.30677  , -0.51606  ,  0.16839  , -0.024237 ,  0.22719  ,\n",
       "        0.018605 ,  0.45249  ,  0.26825  ,  0.263    ,  0.40592  ,\n",
       "       -0.44148  , -0.060587 , -0.25607  , -0.44626  , -0.10527  ,\n",
       "       -0.66607  ,  0.48883  , -0.54111  ,  0.1403   , -0.0476   ,\n",
       "       -0.20509  , -0.36993  ,  0.15421  , -0.093329 ,  0.4347   ,\n",
       "        0.14337  ,  0.052247 , -0.17844  , -0.1292   , -0.18123  ,\n",
       "        0.15523  , -0.14887  , -0.058583 ,  0.40293  ,  1.0223   ,\n",
       "        0.0631   , -0.28373  , -0.14486  , -0.014079 , -0.4358   ,\n",
       "       -0.46028  , -0.70722  ,  0.69638  , -0.035604 , -0.18943  ,\n",
       "       -0.30317  ,  0.080057 , -0.21485  ,  0.10918  ,  0.28817  ,\n",
       "       -0.07454  , -0.079632 ,  0.452    ,  0.011182 ,  0.4781   ,\n",
       "       -0.25833  ,  0.34898  , -0.016316 ,  0.092757 , -0.37827  ,\n",
       "       -0.27047  , -0.03206  ,  0.30495  , -0.18112  , -0.023923 ,\n",
       "       -0.46283  ,  0.40218  ,  0.25496  ,  0.03829  , -0.16211  ,\n",
       "       -0.029873 , -0.033979 ,  0.16065  , -0.26032  , -0.24203  ,\n",
       "        0.12691  , -0.31384  ,  0.48393  , -0.73027  , -0.24479  ,\n",
       "       -0.30343  ,  0.19906  ,  0.12204  ,  0.38072  ,  0.509    ,\n",
       "        0.15137  , -0.52911  ,  0.065863 , -0.3758   , -0.1274   ,\n",
       "        0.080856 ,  0.11545  ,  0.40141  , -0.21268  ,  0.71381  ,\n",
       "        0.1793   ,  0.13791  , -0.37751  ,  0.24224  , -0.12526  ,\n",
       "        0.24988  , -0.39193  , -0.23447  ,  0.38872  ,  0.96019  ,\n",
       "       -0.033372 , -0.08502  , -0.374    , -0.44978  ,  0.80095  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[37984]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('embedding_matrix4',weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "token_captions = []\n",
    "for capt in captions_shuffled:\n",
    "    token_caption = []\n",
    "    token_caption.append(word2idx['<S>'])\n",
    "    words = re.findall(r\"[\\w']+|[.,!?;'><(){}%$#£@-_+=|\\/~`^&*]\", capt)\n",
    "    for word in words:\n",
    "        try:\n",
    "            token = word2idx[word]\n",
    "        except KeyError:\n",
    "            token = word2idx['UNK']\n",
    "        token_caption.append(token)\n",
    "    token_caption.append(word2idx['</S>'])\n",
    "    token_captions.append(token_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38518, 1511, 318, 6315, 41, 113, 690, 4814, 37, 156, 30, 1043, 41, 113, 38519]\n",
      "score against barcelona they said else terry will take your wife they said\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38517"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(token_captions[330000])\n",
    "print(captions_shuffled[330000])\n",
    "word2idx['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "memes_shuffled = list(memes_shuffled)\n",
    "captions_shuffled = list(captions_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "deleters = []\n",
    "for i,ting in enumerate(token_captions):\n",
    "    if len(ting) == 2:\n",
    "        deleters.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76645\n",
      "96449\n",
      "123663\n",
      "135465\n",
      "185376\n",
      "187451\n",
      "233822\n",
      "267491\n",
      "327390\n",
      "372006\n",
      "374966\n",
      "384502\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,ting in enumerate(deleters):\n",
    "    print(ting)\n",
    "    del captions_shuffled[ting-i]\n",
    "    del memes_shuffled[ting-i]\n",
    "    del token_captions[ting-i]\n",
    "deleters = []\n",
    "for i,ting in enumerate(token_captions):\n",
    "    if len(ting) == 2:\n",
    "        deleters.append(i)\n",
    "len(deleters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31996\n"
     ]
    }
   ],
   "source": [
    "many_unk = []\n",
    "for i,capt in enumerate(token_captions):\n",
    "    unk_counter = 0\n",
    "    for token in capt:\n",
    "        if token == word2idx['UNK']:\n",
    "            unk_counter += 1\n",
    "    if unk_counter >= 2:\n",
    "        many_unk.append(i)\n",
    "print(len(many_unk))\n",
    "\n",
    "for i,ting in enumerate(many_unk):\n",
    "    del captions_shuffled[ting-i]\n",
    "    del memes_shuffled[ting-i]\n",
    "    del token_captions[ting-i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353700"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(memes_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Wrapper for inserting an int64 Feature into a SequenceExample proto.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Wrapper for inserting a bytes Feature into a SequenceExample proto.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _int64_feature_list(values):\n",
    "    \"\"\"Wrapper for inserting an int64 FeatureList into a SequenceExample proto.\"\"\"\n",
    "    return tf.train.FeatureList(feature=[_int64_feature(v) for v in values])\n",
    "\n",
    "\n",
    "def _bytes_feature_list(values):\n",
    "    \"\"\"Wrapper for inserting a bytes FeatureList into a SequenceExample proto.\"\"\"\n",
    "    return tf.train.FeatureList(feature=[_bytes_feature(v) for v in values])\n",
    "\n",
    "def _floats_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                0                 0                 0                 0\n",
      "                 0                 0                 0                 0\n",
      "                 0                 0  1886732697486877                 0\n",
      "                 0                 0                 0                 0\n",
      "                 0                 0                 0                 0\n",
      "  5322175502777100                 0                 0                 0\n",
      "                 0                 0  5142025470733643                 0\n",
      "                 0 10490139007568360                 0                 0\n",
      "  8445826530456543                 0   262790709733963                 0\n",
      "                 0                 0                 0                 0\n",
      "                 0                 0  5527552127838135                 0\n",
      "                 0                 0                 0                 0\n",
      "                 0                 0                 0                 0\n",
      "                 0                 0                 0    87184675037860\n",
      "                 0                 0                 0                 0\n",
      " 19245283126831056                 0                 0                 0\n",
      "                 0                 0    38763269782066                 0\n",
      "                 0                 0 10241703033447266 17985334396362304\n",
      "                 0                 0                 0                 0\n",
      "                 0                 0                 0                 0\n",
      "                 0                 0  2479514122009277  1964029192924499\n",
      "                 0                 0                 0                 0\n",
      "                 0                 0                 0                 0\n",
      "                 0                 0                 0                 0\n",
      "                 0                 0  2026408672332763  8542483329772949]\n"
     ]
    }
   ],
   "source": [
    "memes_shuffled_int = []\n",
    "for i,meme in enumerate(memes_shuffled):\n",
    "    memes_shuffled_int.append(np.int_(meme*1000000000000000))\n",
    "print(memes_shuffled_int[20][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'duct tape turning no into \"hmmmnnmmmnn\"\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(captions_shuffled[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 0/353700\n",
      "Train data: 20000/353700\n",
      "Train data: 40000/353700\n",
      "Train data: 60000/353700\n",
      "Train data: 80000/353700\n",
      "Train data: 100000/353700\n",
      "Train data: 120000/353700\n",
      "Train data: 140000/353700\n",
      "Train data: 160000/353700\n",
      "Train data: 180000/353700\n",
      "Train data: 200000/353700\n",
      "Train data: 220000/353700\n",
      "Train data: 240000/353700\n",
      "Train data: 260000/353700\n",
      "Train data: 280000/353700\n",
      "Train data: 300000/353700\n",
      "Train data: 320000/353700\n",
      "Train data: 340000/353700\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "train_filename = 'train.tfrecords6'  # address to save the TFRecords file\n",
    "# open the TFRecords file\n",
    "writer = tf.python_io.TFRecordWriter(train_filename)\n",
    "for i in range(len(memes_shuffled_int)):\n",
    "    if not i % 20000:\n",
    "        print 'Train data: {}/{}'.format(i, len(memes_shuffled_int))\n",
    "        sys.stdout.flush()\n",
    "    context = tf.train.Features(feature={\n",
    "          \"train/meme\": _bytes_feature(memes_shuffled_int[i].tostring()),  #this is the part that needs to be a float save\n",
    "      })\n",
    "    feature_lists = tf.train.FeatureLists(feature_list={\n",
    "          \"train/captions\": _int64_feature_list(token_captions[i])\n",
    "      })\n",
    "    sequence_example = tf.train.SequenceExample(\n",
    "          context=context, feature_lists=feature_lists)\n",
    "    \n",
    "    writer.write(sequence_example.SerializeToString())\n",
    "    \n",
    "writer.close()\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "489\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "for capt in captions_shuffled:\n",
    "    if 'what do we say to' in capt:\n",
    "        #print(capt)\n",
    "        num+=1\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(enumerate(['a','b','c','d','e']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'a'), (1, 'b'), (2, 'c'), (3, 'd'), (4, 'e')]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'a'), (2, 'c')]\n"
     ]
    }
   ],
   "source": [
    "for i,wp in enumerate(x):\n",
    "    if wp[0] == 1:\n",
    "        del x[i]\n",
    "x = x[0:2]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.gfile.GFile('vocab2.txt', mode=\"r\") as f:\n",
    "      wordidx_pairs = list(f.readlines())\n",
    "wordidx_pairs = [(line.split()[0],line.split()[1]) for line in wordidx_pairs]\n",
    "vocab = dict([(x, int(y)) for (x, y) in wordidx_pairs]) #changed this to reflect vocab.txt format\n",
    "x = sorted(vocab.iteritems(), key=lambda x: int(x[1]))\n",
    "reverse_vocab = [y[0] for y in x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vocab[','])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
