{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#some basic imports and setups\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n",
    "\n",
    "#mean of imagenet dataset in BGR\n",
    "imagenet_mean = np.array([104., 117., 124.], dtype=np.float32)\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "image_dir = os.path.join(current_dir, 'memes')\n",
    "#image_dir = current_dir\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from alexnet import AlexNet\n",
    "\n",
    "#placeholder for input and dropout rate\n",
    "x = tf.placeholder(tf.float32, [1, 227, 227, 3])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "#create model with default config ( == no skip_layer and 1000 units in the last layer)\n",
    "model = AlexNet(x, keep_prob, 1000,[],['fc7','fc8'],512,weights_path='bvlc_alexnet.npy') #maybe need to put fc8 in skip_layers\n",
    "\n",
    "#define activation of last layer as score\n",
    "score = model.fc6\n",
    "\n",
    "#create op to calculate softmax \n",
    "#softmax = tf.nn.softmax(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Converting captions and meme vector representations into single Tfrecord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requires putting memes through alexnet to find their vector rep, shuffling the captions, changing captions into their word2idx, finally saving one caption together with one meme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/data/alpv95/MemeProject/im2txt/memes/y-u-no.jpg', 'memes/mendozameme.jpg')\n",
      "(0, 194, 194, 194)\n",
      "(100, 19500, 19500, 19500)\n",
      "sizing error\n",
      "(200, 38642, 38642, 38642)\n",
      "sizing error\n",
      "sizing error\n",
      "sizing error\n",
      "(300, 57715, 57715, 57715)\n",
      "sizing error\n",
      "sizing error\n",
      "(400, 76329, 76329, 76329)\n",
      "sizing error\n",
      "sizing error\n",
      "(500, 94591, 94591, 94591)\n",
      "sizing error\n",
      "sizing error\n",
      "(600, 113146, 113146, 113146)\n",
      "sizing error\n",
      "(700, 131507, 131507, 131507)\n",
      "sizing error\n",
      "(800, 149520, 149520, 149520)\n",
      "sizing error\n",
      "sizing error\n",
      "sizing error\n",
      "(900, 167518, 167518, 167518)\n",
      "(1000, 185425, 185425, 185425)\n",
      "sizing error\n",
      "(1100, 203294, 203294, 203294)\n",
      "(1200, 220336, 220336, 220336)\n",
      "sizing error\n",
      "(1300, 237823, 237823, 237823)\n",
      "sizing error\n",
      "sizing error\n",
      "(1400, 254129, 254129, 254129)\n",
      "sizing error\n",
      "sizing error\n",
      "sizing error\n",
      "(1500, 270058, 270058, 270058)\n",
      "(1600, 286622, 286622, 286622)\n",
      "sizing error\n",
      "sizing error\n",
      "(1700, 302200, 302200, 302200)\n",
      "sizing error\n",
      "(1800, 316652, 316652, 316652)\n",
      "sizing error\n",
      "(1900, 331886, 331886, 331886)\n",
      "(2000, 346319, 346319, 346319)\n",
      "sizing error\n",
      "sizing error\n",
      "(2100, 362089, 362089, 362089)\n",
      "(2200, 375126, 375126, 375126)\n",
      "sizing error\n",
      "sizing error\n",
      "(2300, 389123, 389123, 389123)\n",
      "sizing error\n",
      "sizing error\n",
      "(2400, 401660, 401660, 401660)\n",
      "(2500, 414389, 414389, 414389)\n",
      "23\n",
      "414389\n"
     ]
    }
   ],
   "source": [
    "#USE FOR ALEXNET !!!!!!!!!\n",
    "#TRAINING SET\n",
    "\n",
    "with open('ordered_memes.txt','r') as f:\n",
    "    img_files = f.readlines()\n",
    "img_files = [os.path.join(image_dir, f) for f in img_files] # add path to each file\n",
    "img_files = [img_file.replace('\\n','') for img_file in img_files]\n",
    "print(img_files[0],img_files[-1])\n",
    "with open('Captions.txt','r') as f:\n",
    "    captions = f.readlines()\n",
    "#captions = list(set(captions))\n",
    "captions = [s.lower() for s in captions]\n",
    "deleters = []\n",
    "for i,capt in enumerate(captions):\n",
    "    if ' - ' not in capt or ' - -' in capt:\n",
    "        deleters.append(i)\n",
    "for i,delete in enumerate(deleters):\n",
    "    del captions[delete-i]\n",
    "data_memes = []\n",
    "data_captions = []\n",
    "data_meme_names = [] #just to check captions have been paired correctly\n",
    "counter = 0\n",
    "passed = 0\n",
    "\n",
    "\n",
    "#Doing everything in one script: (the fc6 vectors are quite sparse)\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Load the pretrained weights into the model\n",
    "    model.load_initial_weights(sess)\n",
    "    \n",
    "    for i,meme in enumerate(img_files):\n",
    "        #meme_name = meme.replace('/Users/ALP/Desktop/Stanford/CS224n/MemeProject/memes/','')\n",
    "        #meme_name = meme_name.replace('.jpg','').lower()\n",
    "        #meme_name = meme_name.replace('-',' ')\n",
    "        img = Image.open(meme)\n",
    "        try:\n",
    "            img.thumbnail((227, 227), Image.ANTIALIAS)\n",
    "            #img = img.resize((227,227))\n",
    "            #use img.thumbnail for square images, img.resize for non square\n",
    "            assert np.shape(img) == (227, 227, 3)\n",
    "        except AssertionError:\n",
    "            img = img.resize((227,227))\n",
    "            print('sizing error')\n",
    "        \n",
    "        # Subtract the ImageNet mean\n",
    "        img = img - imagenet_mean #should probably change this\n",
    "        \n",
    "        # Reshape as needed to feed into model\n",
    "        img = img.reshape((1,227,227,3))\n",
    "\n",
    "        meme_vector = sess.run(score, feed_dict={x: img, keep_prob: 1}) #[1,4096]\n",
    "        meme_vector = np.reshape(meme_vector,[4096])\n",
    "        assert np.shape(meme_vector) == (4096,)\n",
    "        \n",
    "        match = []\n",
    "        meme_name = captions[counter].split(' - ')[0]\n",
    "        image_name = img_files[i].replace('/data2/alpv95/MemeProject/im2txt/memes/','')\n",
    "        image_name = image_name.replace('.jpg','')\n",
    "        image_name = image_name.replace('-',' ')\n",
    "        #print(meme_name,image_name)\n",
    "        try:\n",
    "            assert SequenceMatcher(a=meme_name.replace(' ',''),b=image_name.replace(' ','')).ratio() >= 0.75 or image_name in meme_name or meme_name in image_name\n",
    "        except AssertionError:\n",
    "            passed+=1\n",
    "            continue\n",
    "        \n",
    "        while SequenceMatcher(a=meme_name.replace(' ',''),b=captions[counter].split(' - ')[0].replace(' ','')).ratio() >= 0.75 or meme_name in captions[counter].split(' - ')[0] or captions[counter].split(' - ')[0] in meme_name: \n",
    "            if counter==len(captions)-1:\n",
    "                match.append(captions[counter].split(' - ')[-1])\n",
    "                break\n",
    "            elif captions[counter] == captions[counter].split(' - ')[-1]:\n",
    "                counter += 1\n",
    "            else:\n",
    "                match.append(captions[counter].split(' - ')[-1])\n",
    "                counter += 1\n",
    "                \n",
    "        \n",
    "        #now save in tfrecords format, or prepare for that action\n",
    "        meme_vectors = [meme_vector for cap in match]\n",
    "        image_names = [image_name for cap in match]\n",
    "        assert len(meme_vectors) == len(match)\n",
    "        data_memes.extend(meme_vectors)\n",
    "        data_captions.extend(match)\n",
    "        data_meme_names.extend(image_names)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(i,len(data_memes),len(data_captions),len(data_meme_names))\n",
    "\n",
    "print(passed)\n",
    "print(len(data_memes))\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/data2/alpv95/MemeProject/im2txt/memes/y-u-no.jpg', '/data2/alpv95/MemeProject/im2txt/memes/mendozameme.jpg')\n",
      "(0, 194, 194, 194)\n",
      "(100, 19500, 19500, 195)\n",
      "(200, 38642, 38642, 186)\n",
      "(300, 57715, 57715, 193)\n",
      "(400, 76329, 76329, 193)\n",
      "(500, 94590, 94590, 195)\n",
      "(600, 113145, 113145, 193)\n",
      "(700, 131506, 131506, 193)\n",
      "(800, 149519, 149519, 188)\n",
      "(900, 167517, 167517, 145)\n",
      "(1000, 185424, 185424, 182)\n",
      "(1100, 203293, 203293, 178)\n",
      "(1200, 220335, 220335, 191)\n",
      "(1300, 237822, 237822, 193)\n",
      "(1400, 254128, 254128, 188)\n",
      "(1500, 270057, 270057, 106)\n",
      "(1600, 286621, 286621, 190)\n",
      "(1700, 302199, 302199, 135)\n",
      "(1800, 316651, 316651, 191)\n",
      "(1900, 331885, 331885, 195)\n",
      "(2000, 346318, 346318, 192)\n",
      "(2100, 362088, 362088, 1)\n",
      "(2200, 375125, 375125, 188)\n",
      "(2300, 389122, 389122, 23)\n",
      "(2400, 401659, 401659, 189)\n",
      "(2500, 414388, 414388, 1)\n",
      "23\n",
      "414388\n"
     ]
    }
   ],
   "source": [
    "#USE FOR INCEPTION NETWORK (OR ALEXNET WITH FINETUNING) #NOW WITH LABELS INCLUDED\n",
    "#TRAINING SET\n",
    "#For this case just need to save image filenames alongside captions\n",
    "with open('/data2/alpv95/MemeProject/im2txt/ordered_memes.txt','r') as f:\n",
    "    img_files = f.readlines()\n",
    "img_files = [os.path.join(image_dir, f) for f in img_files] # add path to each file\n",
    "img_files = [img_file.replace('\\n','') for img_file in img_files]\n",
    "print(img_files[0],img_files[-1])\n",
    "with open('/data2/alpv95/MemeProject/im2txt/CaptionsClean.txt','r') as f:\n",
    "    captions = f.readlines()\n",
    "\n",
    "data_memes = []\n",
    "data_captions = []\n",
    "data_labels = []\n",
    "counter = 0\n",
    "passed = 0\n",
    "\n",
    "\n",
    "for i,meme in enumerate(img_files):\n",
    "    \n",
    "    match = []\n",
    "    meme_name = captions[counter].split(' - ')[0]\n",
    "    image_name = meme.replace('/data2/alpv95/MemeProject/im2txt/memes/','')\n",
    "    image_name = image_name.replace('.jpg','')\n",
    "    image_name = image_name.replace('-',' ')\n",
    "    #print(meme_name,image_name)\n",
    "    try:\n",
    "        assert SequenceMatcher(a=meme_name.replace(' ',''),b=image_name.replace(' ','')).ratio() >= 0.75 or image_name in meme_name or meme_name in image_name\n",
    "    except AssertionError:\n",
    "        passed+=1\n",
    "        continue\n",
    "        \n",
    "    while SequenceMatcher(a=meme_name.replace(' ',''),b=captions[counter].split(' - ')[0].replace(' ','')).ratio() >= 0.75 or meme_name in captions[counter].split(' - ')[0] or captions[counter].split(' - ')[0] in meme_name: \n",
    "        if counter==len(captions)-1:\n",
    "            match.append(captions[counter].split(' - ')[-1])\n",
    "            break\n",
    "        elif captions[counter] == captions[counter].split(' - ')[-1]:\n",
    "            counter += 1\n",
    "        else:\n",
    "            match.append(captions[counter].split(' - ')[-1])\n",
    "            counter += 1\n",
    "                \n",
    "        \n",
    "    #now save in tfrecords format, or prepare for that action\n",
    "    meme_images = [meme for cap in match]\n",
    "    meme_labels = [meme_name for cap in match]\n",
    "    assert len(meme_images) == len(match)\n",
    "    data_memes.extend(meme_images)\n",
    "    data_captions.extend(match)\n",
    "    data_labels.extend(meme_labels)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(i,len(data_memes),len(data_captions),len(meme_labels))\n",
    "print(passed)\n",
    "print(len(data_memes))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/alpv95/MemeProject/im2txt/memes/i-hardly-know-her.jpg\n",
      "hebrew? i hardly knew you!\n",
      "\n",
      "i hardly know her\n",
      "/data/alpv95/MemeProject/im2txt/memes/typical-grandmother.jpg\n",
      "suor grazia\n",
      "\n",
      "typical grandmother\n",
      "/data/alpv95/MemeProject/im2txt/memes/shabbywag.jpg\n",
      "fuckshitbitch pissassdick\n",
      "\n",
      "shabbywag\n",
      "/data/alpv95/MemeProject/im2txt/memes/facebook-roleplay-ocelot.jpg\n",
      "i not understand your speking inglish rite?\n",
      "\n",
      "facebook roleplay ocelot\n",
      "/data/alpv95/MemeProject/im2txt/memes/facebook-roleplay-ocelot.jpg\n",
      "happy new year alex!! miss & love ya bunches!! <3\n",
      "\n",
      "facebook roleplay ocelot\n",
      "/data/alpv95/MemeProject/im2txt/memes/funny-stupid.jpg\n",
      "and make good choices\n",
      "\n",
      "funny stupid\n",
      "/data/alpv95/MemeProject/im2txt/memes/unlucky-person.jpg\n",
      "egd idag och imorgon\n",
      "\n",
      "unlucky person\n",
      "/data/alpv95/MemeProject/im2txt/memes/indian-gangster-wannabe.jpg\n",
      "my name is anoop aka anoop dogg\n",
      "\n",
      "indian gangster wannabe\n",
      "/data/alpv95/MemeProject/im2txt/memes/indian-gangster-wannabe.jpg\n",
      "gua miss sama lu ah anak ayamkuuu\n",
      "\n",
      "indian gangster wannabe\n",
      "/data/alpv95/MemeProject/im2txt/memes/happy-golfer.jpg\n",
      "trusted instincts always wins\n",
      "\n",
      "happy golfer\n",
      "/data/alpv95/MemeProject/im2txt/memes/happy-golfer.jpg\n",
      "snow day yes!!!!!!\n",
      "\n",
      "happy golfer\n",
      "/data/alpv95/MemeProject/im2txt/memes/lamenting-lemur.jpg\n",
      "dont eat your cake unless you want to poop it all out\n",
      "\n",
      "lamenting lemur\n",
      "/data/alpv95/MemeProject/im2txt/memes/lamenting-lemur.jpg\n",
      "i think i would be cooler... if i shared the same birthday as jon\n",
      "\n",
      "lamenting lemur\n",
      "/data/alpv95/MemeProject/im2txt/memes/epic-beard-man.jpg\n",
      "wtf dude\n",
      "\n",
      "epic beard man\n",
      "/data/alpv95/MemeProject/im2txt/memes/epic-beard-man.jpg\n",
      "beards... cause fuck chin's\n",
      "\n",
      "epic beard man\n",
      "/data/alpv95/MemeProject/im2txt/memes/2006scape.jpg\n",
      "icecream is offisive lock all threads!\n",
      "\n",
      "2006scape!\n",
      "/data/alpv95/MemeProject/im2txt/memes/2006scape.jpg\n",
      "july 2013... our busiest month yet..\n",
      "\n",
      "2006scape!\n",
      "/data/alpv95/MemeProject/im2txt/memes/cryingbreivik.jpg\n",
      "your telling me jocelyn hasn't shaved bitch is getting a razor for christmas\n",
      "\n",
      "cryingbreivik\n",
      "/data/alpv95/MemeProject/im2txt/memes/misunderstood-russia.jpg\n",
      "bludgeons you over the head with a pillow during the pillow fight\n",
      "\n",
      "misunderstood russia\n",
      "/data/alpv95/MemeProject/im2txt/memes/misunderstood-russia.jpg\n",
      "i will beat you  in a game of tetris\n",
      "\n",
      "misunderstood russia\n",
      "/data/alpv95/MemeProject/im2txt/memes/lime-guy.jpg\n",
      "why can't i, hold all these grudges\n",
      "\n",
      "lime guy\n",
      "/data/alpv95/MemeProject/im2txt/memes/lime-guy.jpg\n",
      "why can't i hold all opening these packs\n",
      "\n",
      "lime guy\n",
      "/data/alpv95/MemeProject/im2txt/memes/cynical-animeshniki.jpg\n",
      "language of music extension? not f*cking likely\n",
      "\n",
      "cynical animeshniki\n",
      "/data/alpv95/MemeProject/im2txt/memes/whipped-boyfriend-perry.jpg\n",
      "gets his band's single to number 1 on the charts \"pheobe\" by \"pheobe\"\n",
      "\n",
      "whipped boyfriend perry\n",
      "/data/alpv95/MemeProject/im2txt/memes/whipped-boyfriend-perry.jpg\n",
      "my mates are like cream whipped!\n",
      "\n",
      "whipped boyfriend perry\n",
      "/data/alpv95/MemeProject/im2txt/memes/hannibalbarca13.jpg\n",
      "i hate capitalism and eating less than 8000 calories per meal\n",
      "\n",
      "hannibalbarca13\n",
      "/data/alpv95/MemeProject/im2txt/memes/hannibalbarca13.jpg\n",
      "oh yeah? costco huh?  cool story bro\n",
      "\n",
      "hannibalbarca13\n",
      "/data/alpv95/MemeProject/im2txt/memes/bandwagon-baboon.jpg\n",
      "leigh is a babbling baboon\n",
      "\n",
      "bandwagon baboon\n",
      "/data/alpv95/MemeProject/im2txt/memes/bandwagon-baboon.jpg\n",
      "baboons breaking through windows south african problems\n",
      "\n",
      "bandwagon baboon\n",
      "/data/alpv95/MemeProject/im2txt/memes/sumoface.jpg\n",
      "i accidently put yeast  in my head when i was little now look at me\n",
      "\n",
      "sumoface\n",
      "/data/alpv95/MemeProject/im2txt/memes/vesyolyi-pochvoved.jpg\n",
      "after you get a murdoch degree\n",
      "\n",
      "vesyolyi-pochvoved\n",
      "/data/alpv95/MemeProject/im2txt/memes/raging-metal-chick.jpg\n",
      "blommewienkel was wegens omstandigheden gesloten\n",
      "\n",
      "raging metal chick\n",
      "/data/alpv95/MemeProject/im2txt/memes/maple-noob.jpg\n",
      "miky  the house of the mouse\n",
      "\n",
      "maple noob\n",
      "/data/alpv95/MemeProject/im2txt/memes/bad-luck-cyhi.jpg\n",
      "good is the kush      ymcmb is the mid-grade cyhi is the snicklefritz\n",
      "\n",
      "bad luck cyhi\n",
      "/data/alpv95/MemeProject/im2txt/memes/bad-luck-cyhi.jpg\n",
      "cs leaked? pm please\n",
      "\n",
      "bad luck cyhi\n",
      "/data/alpv95/MemeProject/im2txt/memes/misophonia-meerkat.jpg\n",
      "pizza cows\n",
      "\n",
      "misophonia meerkat\n",
      "/data/alpv95/MemeProject/im2txt/memes/misophonia-meerkat.jpg\n",
      "\"just ignore it\" just stop it\n",
      "\n",
      "misophonia meerkat\n",
      "/data/alpv95/MemeProject/im2txt/memes/vk-voditel.jpg\n",
      "mi carrito es ferrari \n",
      "\n",
      "vk_voditel\n",
      "/data/alpv95/MemeProject/im2txt/memes/pans-labyrinth1.jpg\n",
      "oh, you're afraid the weather will ruin your hair? let me get a closer look.  \n",
      "\n",
      "pan's labyrinth1\n",
      "/data/alpv95/MemeProject/im2txt/memes/pans-labyrinth1.jpg\n",
      "i'll have the sandusky-tini. \n",
      "\n",
      "pan's labyrinth1\n"
     ]
    }
   ],
   "source": [
    "for i in range(10,50):\n",
    "    print(data_memes[i*100+402000])\n",
    "    print(data_captions[i*100+402000])\n",
    "    print(data_labels[i*100+402000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arashian alpaca'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_labels[len(data_memes)-200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n",
      "['yo dawg', 'prepare yourselves', 'so your', 'not sure', 'guess who', 'you said', 'back in', \"i'll have\", 'yeah, if', 'you get', \"i'll start\", \"so you'r\", 'i dont', 'look marge', 'look at', 'in soviet', 'remeber when', 'if you', 'hey girl', \"why can't\", 'i thought', 'why the', 'i can', \"why don't\", 'my precious', 'this is', 'confucius say', 'brace yourselves', 'how tough', 'i see', 'what do', \"i don't\", 'and then', 'i said:', 'good good', 'si no', 'watch out', 'welcome to', 'yeah sure', 'i hate', 'yo mama', 'did somebody', 'everyone is', 'good morning', 'when you', 'its not', 'right in', 'you merely', 'remember when', 'hide yo', 'i fucking', 'happy birthday', 'niggas be', 'uy si', 'vote for', 'por los', 'nigga you', \"and that's\", 'no one', \"if you're\", 'oogah boogah', 'hoy en', 'you mean', 'oh so', \"you don't\", 'so you', 'er mah', 'hey guys', 'i heard', 'dafuq did', \"you've just\", 'there will', 'fuck you', 'i like', 'release the', 'you should', 'fuck it', 'no i', \"i'm commander\", 'ah, yes,', 'well done', \"i'm in\", 'kyriarchy wasp', 'aw yiss', 'you think', 'real men', 'you want', 'met a', 'ho stato', 'stop trying', 'oh you', 'show no', 'im going', 'penis for', 'instant flame', 'cannot into', 'well play']\n"
     ]
    }
   ],
   "source": [
    "#CREATE EVALUATION SET    #NOW WITH LABELS AS WELL\n",
    "#SHOULD BE MEMES WITH REPEATED FORMAT \n",
    "counter = 0\n",
    "R = []\n",
    "idxs = []\n",
    "for i,capt in enumerate(data_captions):\n",
    "    if i % 190 == 0: \n",
    "        start = ' '.join(capt.split()[0:2])\n",
    "        counter = 0\n",
    "    if SequenceMatcher(a=start,b=' '.join(capt.split()[0:2])).ratio() >= 0.9:\n",
    "        counter += 1 \n",
    "    if counter >20 and start not in R:\n",
    "        R.append(start)\n",
    "        idxs.append(i)\n",
    "print(len(R))\n",
    "\n",
    "deleters = [0,1,3,16,79,99]\n",
    "for i,ting in enumerate(deleters):\n",
    "    del R[ting-i]\n",
    "    del idxs[ting-i]\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_examples = [0,570,760,1140,2850,3040,4940,28500] + idxs\n",
    "eval_captions = ['y u no','i dont always','not sure if','one does not simply','what if i told you','what if',\"so you're telling me\",'so then i said'] + R\n",
    "eval_labels = []\n",
    "eval_memes = []\n",
    "\n",
    "for idx in eval_examples:\n",
    "    eval_memes.append(data_memes[idx])\n",
    "    eval_labels.append(data_labels[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y u no',\n",
       " 'the most interesting man in the world',\n",
       " 'futurama fry',\n",
       " 'one does not simply',\n",
       " 'what if i told you',\n",
       " 'conspiracy keanu',\n",
       " 'skeptical 3rd world kid',\n",
       " 'so then i said...',\n",
       " 'yo dawg',\n",
       " 'prepare yourself',\n",
       " 'skeptical african child',\n",
       " 'not sure if troll',\n",
       " 'guess who ?',\n",
       " 'maury lie detector',\n",
       " '1889 [10] guy',\n",
       " 'tough spongebob',\n",
       " 'yeah if you could just',\n",
       " 'oprah you get a',\n",
       " 'bender blackjack and hookers',\n",
       " 'skeptical third-world kid',\n",
       " \"i don't know who you are...\",\n",
       " 'look-marge',\n",
       " 'look at all the fucks i give',\n",
       " 'in soviet russia',\n",
       " 'pepperidge farm remembers fg',\n",
       " 'office space boss',\n",
       " 'ryan gosling hey girl 3',\n",
       " \"why can't i hold all these?!?!?\",\n",
       " 'never have i been so wrong',\n",
       " 'patrick stewart wtf',\n",
       " 'i can haz',\n",
       " 'pushing patrick',\n",
       " 'my precious gollum',\n",
       " \"timmy turner's dad if i had one!\",\n",
       " 'wise confucius',\n",
       " 'brace yourselves.',\n",
       " 'how tough are you',\n",
       " 'i see dead people',\n",
       " 'what do we say to the god of death ? ',\n",
       " 'serj tankian',\n",
       " 'reagan white house laughing',\n",
       " 'cm punk apologize!',\n",
       " 'darth sidious mun',\n",
       " 'hay tabla',\n",
       " 'neil degrasse tyson reaction',\n",
       " 'whose line',\n",
       " 'yeah....sure',\n",
       " 'contradictory chris',\n",
       " 'yo mama',\n",
       " 'inappropriate timing bill clinton',\n",
       " 'and im just sitting here masterbating',\n",
       " 'good morning son',\n",
       " 'only then you have my permission to die',\n",
       " \"it's about sending a message\",\n",
       " 'right in the childhood man',\n",
       " 'born in it bane',\n",
       " 'family guy pepperidge farm',\n",
       " 'hide your kids',\n",
       " 'i fucking love ',\n",
       " 'happy birthday black kid',\n",
       " 'niggas be like',\n",
       " 'uy si uy si uy',\n",
       " 'vote for pedro',\n",
       " 'tecate',\n",
       " 'nigga, you just went full retard',\n",
       " 'j walter weatherman',\n",
       " 'like gaston',\n",
       " 'super cool ski instructor',\n",
       " 'oogah boogah',\n",
       " 'iker jimenez',\n",
       " 'dubious history teacher',\n",
       " \"oh so you're\",\n",
       " \"you're wrong and i hate you\",\n",
       " 'poor black kid',\n",
       " 'er mah gerd',\n",
       " 'slowbro',\n",
       " 'i heard fifa 13 is so real',\n",
       " 'dafuq',\n",
       " 'you just activated my trap card',\n",
       " 'there will be blood',\n",
       " 'ted fuck you thunder',\n",
       " 'i like turtles ',\n",
       " 'release the kraken',\n",
       " 'avengers pointing',\n",
       " \"angry bill o'reilly\",\n",
       " 'angry dj',\n",
       " 'blatant commander shepard',\n",
       " 'ah, yes, reapers',\n",
       " 'neville southall',\n",
       " 'kai mountain climber',\n",
       " 'kyriarchy wasp',\n",
       " 'aww yiss',\n",
       " 'motherfucking game cat',\n",
       " 'old man river',\n",
       " 'valentina mehailovna',\n",
       " 'met a smosh fan',\n",
       " 'michele misseri',\n",
       " 'regina george ',\n",
       " 'douchebag wolverine ',\n",
       " 'dictator cat',\n",
       " 'rainbowdash',\n",
       " 'tom anex',\n",
       " 'instant flame war',\n",
       " 'poland ball',\n",
       " 'well played']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "385707"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Should also look at removing examples which are similar but not exactly the same ##WITH LABELS\n",
    "\n",
    "c = list(zip(data_memes, data_captions, data_labels))\n",
    "no_repeats = []\n",
    "# order preserving\n",
    "def idfun(x): return x\n",
    "\n",
    "seen = {}\n",
    "no_repeats = []\n",
    "for item in c:\n",
    "    marker = idfun(item[1])\n",
    "    # in old Python versions:\n",
    "    # if seen.has_key(marker)\n",
    "    # but in new ones:\n",
    "    if marker in seen: continue\n",
    "    seen[marker] = 1\n",
    "    no_repeats.append(item)\n",
    "print(len(data_captions))\n",
    "len(no_repeats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "shuffle(no_repeats)\n",
    "memes_shuffled, captions_shuffled, labels_shuffled = zip(*no_repeats)\n",
    "memes_shuffled = list(memes_shuffled)\n",
    "captions_shuffled = list(captions_shuffled)\n",
    "labels_shuffled = list(labels_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771414\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "word_captions = []\n",
    "for capt in captions_shuffled + labels_shuffled: #include labels_shuffled here for glove averages\n",
    "    words = re.findall(r\"[\\w']+|[.,:!?;'><(){}%$#£@-_+=|\\/~`^&*]\", capt)\n",
    "    word_captions.append(words)\n",
    "print(len(word_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocabulary.\n",
      "('Total words:', 150760)\n",
      "('Words in vocabulary:', 41819)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(\"Creating vocabulary.\")\n",
    "counter = Counter()\n",
    "for c in word_captions:\n",
    "    counter.update(c)\n",
    "print(\"Total words:\", len(counter))\n",
    "\n",
    "# Filter uncommon words and sort by descending count.\n",
    "word_counts = [x for x in counter.items() if x[1] >= 3]\n",
    "word_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"Words in vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the vocabulary dictionary.\n",
    "reverse_vocab = [x[0] for x in word_counts]\n",
    "#unk_id = len(reverse_vocab)\n",
    "vocab_dict = dict([(x, y) for (y, x) in enumerate(reverse_vocab)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict[':']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "38821\n",
      "('Wrote vocabulary file:', 'vocab_averages.txt')\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIMENSION=300 # Available dimensions for 6B data is 50, 100, 200, 300\n",
    "data_directory = current_dir\n",
    "\n",
    "PAD_TOKEN = 0\n",
    "\n",
    "word2idx = { 'PAD': PAD_TOKEN } # dict so we can lookup indices for tokenising our text later from string to sequence of integers\n",
    "weights = []\n",
    "index_counter = 0\n",
    "\n",
    "with open('glove.42B.300d.txt','r') as file:\n",
    "    for index, line in enumerate(file):\n",
    "        values = line.split() # Word and weights separated by space\n",
    "        word = values[0] # Word is first symbol on each line\n",
    "        if word in vocab_dict:\n",
    "            index_counter += 1\n",
    "            word_weights = np.asarray(values[1:], dtype=np.float32) # Remainder of line is weights for word\n",
    "            word2idx[word] = index_counter # PAD is our zeroth index so shift by one\n",
    "            weights.append(word_weights)\n",
    "        if index % 100000 == 0:\n",
    "            print(index)\n",
    "        if index + 1 == 2000000:\n",
    "            break\n",
    "\n",
    "EMBEDDING_DIMENSION = len(weights[0])\n",
    "# Insert the PAD weights at index 0 now we know the embedding dimension\n",
    "weights.insert(0, np.zeros(EMBEDDING_DIMENSION))\n",
    "\n",
    "# Append unknown and pad to end of vocab and initialize as random #maybe include start and end token here\n",
    "UNKNOWN_TOKEN=len(weights)\n",
    "#include * token as its very common? why not\n",
    "word2idx['*'] = UNKNOWN_TOKEN\n",
    "word2idx['UNK'] = UNKNOWN_TOKEN + 1\n",
    "word2idx['<S>'] = UNKNOWN_TOKEN + 2\n",
    "word2idx['</S>'] = UNKNOWN_TOKEN + 3\n",
    "weights.append(np.random.randn(EMBEDDING_DIMENSION)*0.5)\n",
    "weights.append(np.random.randn(EMBEDDING_DIMENSION)*0.5)\n",
    "weights.append(np.random.randn(EMBEDDING_DIMENSION)*0.5)\n",
    "weights.append(np.random.randn(EMBEDDING_DIMENSION)*0.5)\n",
    "\n",
    "# Construct our final vocab\n",
    "weights = np.asarray(weights, dtype=np.float32)\n",
    "\n",
    "VOCAB_SIZE=weights.shape[0]\n",
    "print(VOCAB_SIZE)\n",
    "\n",
    "#Save Vocabulary IF NEW\n",
    "with tf.gfile.FastGFile('vocab_averages.txt', \"w\") as f:\n",
    "    f.write(\"\\n\".join([\"%s %d\" % (w, c) for w, c in word2idx.iteritems()]))\n",
    "print(\"Wrote vocabulary file:\", 'vocab_averages.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights[37984]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "766"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx['_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SAVE EMBEDDING MATRIX IF NEW\n",
    "np.savetxt('embedding_matrix_averages',weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "token_captions = []\n",
    "for capt in captions_shuffled:\n",
    "    token_caption = []\n",
    "    token_caption.append(word2idx['<S>'])\n",
    "    words = re.findall(r\"[\\w']+|[.,:!?;'><(){}%$#£@-_+=|\\/~`^&*]\", capt)\n",
    "    for word in words:\n",
    "        try:\n",
    "            token = word2idx[word]\n",
    "        except KeyError:\n",
    "            token = word2idx['UNK']\n",
    "        token_caption.append(token)\n",
    "    token_caption.append(word2idx['</S>'])\n",
    "    token_captions.append(token_caption)\n",
    "    \n",
    "#for training labels\n",
    "token_labels = []\n",
    "for capt in labels_shuffled:\n",
    "    token_caption = []\n",
    "    words = re.findall(r\"[\\w']+|[.,:!?;'><(){}%$#£@-_+=|\\/~`^&*]\", capt)\n",
    "    for word in words:\n",
    "        try:\n",
    "            token = word2idx[word]\n",
    "        except KeyError:\n",
    "            token = word2idx['UNK']\n",
    "        token_caption.append(token)\n",
    "    token_labels.append(token_caption)\n",
    "    \n",
    "#for eval set\n",
    "eval_tokens = []\n",
    "for capt in eval_captions:\n",
    "    token_caption = []\n",
    "    token_caption.append(word2idx['<S>'])\n",
    "    words = re.findall(r\"[\\w']+|[.,:!?;'><(){}%$#£@-_+=|\\/~`^&*]\", capt)\n",
    "    for word in words:\n",
    "        try:\n",
    "            token = word2idx[word]\n",
    "        except KeyError:\n",
    "            token = word2idx['UNK']\n",
    "        token_caption.append(token)\n",
    "    #token_caption.append(word2idx['</S>'])\n",
    "    eval_tokens.append(token_caption)\n",
    "    \n",
    "#for eval labels\n",
    "token_labels_eval = []\n",
    "for capt in eval_labels:\n",
    "    token_caption = []\n",
    "    words = re.findall(r\"[\\w']+|[.,:!?;'><(){}%$#£@-_+=|\\/~`^&*]\", capt)\n",
    "    for word in words:\n",
    "        try:\n",
    "            token = word2idx[word]\n",
    "        except KeyError:\n",
    "            token = word2idx['UNK']\n",
    "        token_caption.append(token)\n",
    "    token_labels_eval.append(token_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38819, 967, 11, 21, 9, 2, 1850, 1291, 3070, 8, 2, 512, 271, 136, 1, 43, 32, 141, 868, 226, 3, 695, 905, 11, 819, 1, 1475, 118, 280, 5135, 34, 38820]\n",
      "radio: \"this is the biggest heat wave in the past 20 years, do not go outside today.\" field director: \"guys, pack two water bottles!\"\n",
      "\n",
      "[12451, 655, 813]\n",
      "archaeology major dog\n",
      "[38819, 47, 138, 32, 717]\n",
      "one does not simply\n",
      "[47, 138, 32, 717]\n",
      "one does not simply\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38818"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(token_captions[330000])\n",
    "print(captions_shuffled[330000])\n",
    "print(token_labels[330000])\n",
    "print(labels_shuffled[330000])\n",
    "print(eval_tokens[3])\n",
    "print(eval_captions[3])\n",
    "print(token_labels_eval[3])\n",
    "print(eval_labels[3])\n",
    "word2idx['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "memes_shuffled = list(memes_shuffled)\n",
    "captions_shuffled = list(captions_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deleters = []\n",
    "for i,ting in enumerate(token_captions):\n",
    "    if len(ting) == 2:\n",
    "        deleters.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62361\n",
      "65245\n",
      "116374\n",
      "189587\n",
      "199817\n",
      "252561\n",
      "283794\n",
      "295565\n",
      "301193\n",
      "369659\n",
      "378425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,ting in enumerate(deleters):\n",
    "    print(ting)\n",
    "    del captions_shuffled[ting-i]\n",
    "    del memes_shuffled[ting-i]\n",
    "    del token_captions[ting-i]\n",
    "    del labels_shuffled[ting-i]\n",
    "    del token_labels[ting-i]\n",
    "deleters = []\n",
    "for i,ting in enumerate(token_captions):\n",
    "    if len(ting) == 2:\n",
    "        deleters.append(i)\n",
    "len(deleters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30209\n"
     ]
    }
   ],
   "source": [
    "many_unk = []\n",
    "for i,capt in enumerate(token_captions):\n",
    "    unk_counter = 0\n",
    "    for token in capt:\n",
    "        if token == word2idx['UNK']:\n",
    "            unk_counter += 1\n",
    "    if unk_counter >= 2:\n",
    "        many_unk.append(i)\n",
    "print(len(many_unk))\n",
    "\n",
    "for i,ting in enumerate(many_unk):\n",
    "    del captions_shuffled[ting-i]\n",
    "    del memes_shuffled[ting-i]\n",
    "    del token_captions[ting-i]\n",
    "    del labels_shuffled[ting-i]\n",
    "    del token_labels[ting-i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "733\n"
     ]
    }
   ],
   "source": [
    "unk_label = []\n",
    "for i,capt in enumerate(token_labels):\n",
    "    unk_counter = 0\n",
    "    for token in capt:\n",
    "        if token == word2idx['UNK']:\n",
    "            unk_counter += 1\n",
    "    if unk_counter >= 2:\n",
    "        unk_label.append(i)\n",
    "print(len(unk_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tipichnuy bntu\n",
      "ranzigen peelaert \n",
      "tipichnuy bgtu\n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "vaverka-perakladczyca\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "assburgerin superneuvo\n",
      "vesyolyi-pochvoved\n",
      "tipichnuy bntu\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "typacal viewy\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "typacal viewy\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "vaverka-perakladczyca\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "assburgerin superneuvo\n",
      "vaverka-perakladczyca\n",
      "assburgerin superneuvo\n",
      "pan's labyrinth1\n",
      "vaverka-perakladczyca\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "assburgerin superneuvo\n",
      "vandinha depressao\n",
      "vesyolyi-pochvoved\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "tipichnuy bgtu\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "assburgerin superneuvo\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "suleyman-kerimov\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "tipichnuy bntu\n",
      "tipichnuy bgtu\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "assburgerin superneuvo\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "vaverka-perakladczyca\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "tipichnuy bntu\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "vaverka-perakladczyca\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "tipichnuy bntu\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "vaverka-perakladczyca\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "typacal viewy\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "vaverka-perakladczyca\n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "tipichnuy bgtu\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "vaverka-perakladczyca\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "assburgerin superneuvo\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "tipichnuy bntu\n",
      "vaverka-perakladczyca\n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "assburgerin superneuvo\n",
      "vaverka-perakladczyca\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "tipichnuy bgtu\n",
      "tipichnuy bgtu\n",
      "ranzigen peelaert \n",
      "vesyolyi-pochvoved\n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "typacal viewy\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "typacal viewy\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "assburgerin superneuvo\n",
      "vandinha depressao\n",
      "typacal viewy\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "vesyolyi-pochvoved\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "vaverka-perakladczyca\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "ranzigen peelaert \n",
      "assburgerin superneuvo\n",
      "assburgerin superneuvo\n",
      "tipichnuy bgtu\n",
      "tipichnuy bntu\n",
      "tipichnuy bgtu\n",
      "tipichnuy bgeu\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "vesyolyi-pochvoved\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "tipichnuy bntu\n",
      "ranzigen peelaert \n",
      "tipichnuy bgtu\n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "typacal viewy\n",
      "pan's labyrinth1\n",
      "vaverka-perakladczyca\n",
      "assburgerin superneuvo\n",
      "tipichnuy bntu\n",
      "assburgerin superneuvo\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "assburgerin superneuvo\n",
      "vaverka-perakladczyca\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "assburgerin superneuvo\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "assburgerin superneuvo\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "tipichnuy bgtu\n",
      "typacal viewy\n",
      "vaverka-perakladczyca\n",
      "assburgerin superneuvo\n",
      "pan's labyrinth1\n",
      "assburgerin superneuvo\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "tipichnuy bgtu\n",
      "ranzigen peelaert \n",
      "typacal viewy\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "assburgerin superneuvo\n",
      "vaverka-perakladczyca\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "vesyolyi-pochvoved\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "tipichnuy bgtu\n",
      "ranzigen peelaert \n",
      "typacal viewy\n",
      "ranzigen peelaert \n",
      "tipichnuy bgtu\n",
      "assburgerin superneuvo\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "assburgerin superneuvo\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "vaverka-perakladczyca\n",
      "assburgerin superneuvo\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "vaverka-perakladczyca\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "typacal viewy\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "vaverka-perakladczyca\n",
      "assburgerin superneuvo\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "typacal viewy\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "vaverka-perakladczyca\n",
      "assburgerin superneuvo\n",
      "vandinha depressao\n",
      "tipichnuy bntu\n",
      "assburgerin superneuvo\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "assburgerin superneuvo\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "vaverka-perakladczyca\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "assburgerin superneuvo\n",
      "typacal viewy\n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "vaverka-perakladczyca\n",
      "vandinha depressao\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "typacal viewy\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "tipichnuy bgtu\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "vesyolyi-pochvoved\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "assburgerin superneuvo\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "typacal viewy\n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "vesyolyi-pochvoved\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "vaverka-perakladczyca\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "vesyolyi-pochvoved\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "tipichnuy bntu\n",
      "typacal viewy\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "assburgerin superneuvo\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "assburgerin superneuvo\n",
      "vandinha depressao\n",
      "assburgerin superneuvo\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "tipichnuy bgtu\n",
      "assburgerin superneuvo\n",
      "vandinha depressao\n",
      "assburgerin superneuvo\n",
      "vesyolyi-pochvoved\n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "typacal viewy\n",
      "vaverka-perakladczyca\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "tipichnuy bgtu\n",
      "tipichnuy bntu\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "assburgerin superneuvo\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "assburgerin superneuvo\n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "ranzigen peelaert \n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "assburgerin superneuvo\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "assburgerin superneuvo\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "assburgerin superneuvo\n",
      "tipichnuy bntu\n",
      "ranzigen peelaert \n",
      "assburgerin superneuvo\n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "ranzigen peelaert \n",
      "tipichnuy bgtu\n",
      "tipichnuy bgtu\n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "assburgerin superneuvo\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "tipichnuy bntu\n",
      "assburgerin superneuvo\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "vaverka-perakladczyca\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "tipichnuy bntu\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "tipichnuy bntu\n",
      "tipichnuy bntu\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "pan's labyrinth1\n",
      "tipichnuy bgeu\n",
      "pan's labyrinth1\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "typacal viewy\n",
      "vesyolyi-pochvoved\n",
      "typacal viewy\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "vaverka-perakladczyca\n",
      "tipichnuy bgtu\n",
      "pan's labyrinth1\n",
      "ranzigen peelaert \n",
      "tipichnuy bntu\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "ranzigen peelaert \n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "tipichnuy bgtu\n",
      "tipichnuy bgtu\n",
      "vandinha depressao\n",
      "vandinha depressao\n",
      "pan's labyrinth1\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "orig_unk_labels = []\n",
    "for i in range(len(unk_label)):\n",
    "    print(labels_shuffled[unk_label[i]])\n",
    "    orig_unk_labels.append(labels_shuffled[unk_label[i]])\n",
    "print(len(set(orig_unk_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Wrapper for inserting an int64 Feature into a SequenceExample proto.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Wrapper for inserting a bytes Feature into a SequenceExample proto.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _int64_feature_list(values):\n",
    "    \"\"\"Wrapper for inserting an int64 FeatureList into a SequenceExample proto.\"\"\"\n",
    "    return tf.train.FeatureList(feature=[_int64_feature(v) for v in values])\n",
    "\n",
    "\n",
    "def _bytes_feature_list(values):\n",
    "    \"\"\"Wrapper for inserting a bytes FeatureList into a SequenceExample proto.\"\"\"\n",
    "    return tf.train.FeatureList(feature=[_bytes_feature(v) for v in values])\n",
    "\n",
    "def _floats_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9168698310852050                 0                 0                 0\n",
      "                 0                 0                 0                 0\n",
      "                 0                 0  2523229837417602                 0\n",
      "                 0                 0                 0                 0\n",
      "                 0                 0                 0   308641821146011\n",
      "                 0                 0                 0  9107991218566894\n",
      "                 0                 0  6078919887542725                 0\n",
      "                 0                 0                 0                 0\n",
      "  3697678327560425                 0                 0                 0\n",
      "                 0                 0                 0                 0\n",
      "                 0                 0                 0                 0\n",
      "                 0  3752865076065063                 0                 0\n",
      "                 0                 0  4150147914886474  3943297624588012\n",
      "                 0                 0                 0                 0\n",
      "                 0                 0                 0                 0\n",
      "                 0                 0                 0 18090923309326172\n",
      "                 0                 0                 0   659005880355835\n",
      "  3415115118026733                 0                 0                 0\n",
      " 25201379776000976    58916352689266                 0                 0\n",
      "                 0                 0                 0  2246412992477417\n",
      "                 0                 0  6119893550872803  1267738580703735\n",
      "                 0                 0                 0                 0\n",
      "                 0  1782684445381164                 0  5975249767303467\n",
      "                 0  2116162538528442                 0                 0\n",
      "  1107490420341491 12133914947509766                 0                 0]\n"
     ]
    }
   ],
   "source": [
    "#ONLY FOR ALEXNET \n",
    "memes_shuffled_int = []\n",
    "for i,meme in enumerate(memes_shuffled):\n",
    "    memes_shuffled_int.append(np.int_(meme*1000000000000000))\n",
    "print(memes_shuffled_int[20][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ONLY FOR ALEXNET \n",
    "eval_memes_int = []\n",
    "for i,meme in enumerate(eval_memes):\n",
    "    eval_memes_int.append(np.int_(meme*1000000000000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ONLY FOR INCEPTION\n",
    "class ImageDecoder(object):\n",
    "    \"\"\"Helper class for decoding images in TensorFlow.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Create a single TensorFlow Session for all image decoding calls.\n",
    "        self._sess = tf.Session()\n",
    "\n",
    "        # TensorFlow ops for JPEG decoding.\n",
    "        self._encoded_jpeg = tf.placeholder(dtype=tf.string)\n",
    "        self._decode_jpeg = tf.image.decode_jpeg(self._encoded_jpeg, channels=3)\n",
    "\n",
    "    def decode_jpeg(self, encoded_jpeg):\n",
    "        image = self._sess.run(self._decode_jpeg,\n",
    "                               feed_dict={self._encoded_jpeg: encoded_jpeg})\n",
    "        assert len(image.shape) == 3\n",
    "        assert image.shape[2] == 3\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 0/355437\n",
      "Train data: 20000/355437\n",
      "Train data: 40000/355437\n",
      "Train data: 60000/355437\n",
      "Train data: 80000/355437\n",
      "Train data: 100000/355437\n",
      "Train data: 120000/355437\n",
      "Train data: 140000/355437\n",
      "Train data: 160000/355437\n",
      "Train data: 180000/355437\n",
      "Train data: 200000/355437\n",
      "Train data: 220000/355437\n",
      "Train data: 240000/355437\n",
      "Train data: 260000/355437\n",
      "Train data: 280000/355437\n",
      "Train data: 300000/355437\n",
      "Train data: 320000/355437\n",
      "Train data: 340000/355437\n"
     ]
    }
   ],
   "source": [
    "#SAVING TRAINING SET (ALEXNET)\n",
    "import sys\n",
    "train_filename = 'train.tfrecordsALEX'  # address to save the TFRecords file\n",
    "# open the TFRecords file\n",
    "writer = tf.python_io.TFRecordWriter(train_filename)\n",
    "for i in range(len(memes_shuffled_int)):\n",
    "    if not i % 20000:\n",
    "        print 'Train data: {}/{}'.format(i, len(memes_shuffled_int))\n",
    "        sys.stdout.flush()\n",
    "    context = tf.train.Features(feature={\n",
    "          \"train/meme\": _bytes_feature(memes_shuffled_int[i].tostring()), \n",
    "      })\n",
    "    feature_lists = tf.train.FeatureLists(feature_list={\n",
    "          \"train/captions\": _int64_feature_list(token_captions[i])\n",
    "      })\n",
    "    sequence_example = tf.train.SequenceExample(\n",
    "          context=context, feature_lists=feature_lists)\n",
    "    \n",
    "    writer.write(sequence_example.SerializeToString())\n",
    "    \n",
    "writer.close()\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 0/355487\n",
      "Train data: 20000/355487\n",
      "Train data: 40000/355487\n",
      "Train data: 60000/355487\n",
      "Train data: 80000/355487\n",
      "Train data: 100000/355487\n",
      "Train data: 120000/355487\n",
      "Train data: 140000/355487\n",
      "Train data: 160000/355487\n",
      "Train data: 180000/355487\n",
      "Train data: 200000/355487\n",
      "Train data: 220000/355487\n",
      "Train data: 240000/355487\n",
      "Train data: 260000/355487\n",
      "Train data: 280000/355487\n",
      "Train data: 300000/355487\n",
      "Train data: 320000/355487\n",
      "Train data: 340000/355487\n"
     ]
    }
   ],
   "source": [
    "#SAVING TRAINING SET (INCEPTION)\n",
    "import sys\n",
    "decoder = ImageDecoder()\n",
    "train_filename = 'train.tfrecords'  # address to save the TFRecords file\n",
    "# open the TFRecords file\n",
    "writer = tf.python_io.TFRecordWriter(train_filename)\n",
    "for i in range(len(memes_shuffled)):\n",
    "    \n",
    "    with tf.gfile.FastGFile(memes_shuffled[i], \"r\") as f:\n",
    "        encoded_image = f.read()\n",
    "    \n",
    "    try:\n",
    "        decoder.decode_jpeg(encoded_image)\n",
    "    except (tf.errors.InvalidArgumentError, AssertionError):\n",
    "        print(\"Skipping file with invalid JPEG data: %s\" % image.filename)\n",
    "        break\n",
    "\n",
    "    if not i % 20000:\n",
    "        print 'Train data: {}/{}'.format(i, len(memes_shuffled))\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    context = tf.train.Features(feature={\n",
    "          \"train/meme\": _bytes_feature(encoded_image), \n",
    "      })\n",
    "    feature_lists = tf.train.FeatureLists(feature_list={\n",
    "          \"train/captions\": _int64_feature_list(token_captions[i]),\n",
    "          #\"train/labels\": _int64_feature_list(token_labels[i])\n",
    "      })\n",
    "    sequence_example = tf.train.SequenceExample(\n",
    "          context=context, feature_lists=feature_lists)\n",
    "    \n",
    "    writer.write(sequence_example.SerializeToString())\n",
    "    \n",
    "writer.close()\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 0/8\n"
     ]
    }
   ],
   "source": [
    "#SAVING EVAL SET (ALEXNET)\n",
    "import sys\n",
    "train_filename = 'eval.tfrecordsALEX'  # address to save the TFRecords file\n",
    "# open the TFRecords file\n",
    "writer = tf.python_io.TFRecordWriter(train_filename)\n",
    "for i in range(len(eval_memes_int)):\n",
    "    if not i % 20000:\n",
    "        print 'Train data: {}/{}'.format(i, len(eval_memes_int))\n",
    "        sys.stdout.flush()\n",
    "    context = tf.train.Features(feature={\n",
    "          \"train/meme\": _bytes_feature(eval_memes_int[i].tostring()), \n",
    "      })\n",
    "    feature_lists = tf.train.FeatureLists(feature_list={\n",
    "          \"train/captions\": _int64_feature_list(eval_tokens[i])\n",
    "      })\n",
    "    sequence_example = tf.train.SequenceExample(\n",
    "          context=context, feature_lists=feature_lists)\n",
    "    \n",
    "    writer.write(sequence_example.SerializeToString())\n",
    "    \n",
    "writer.close()\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 0/105\n"
     ]
    }
   ],
   "source": [
    "#SAVING EVAL SET (INCEPTION)\n",
    "import sys\n",
    "decoder = ImageDecoder()\n",
    "train_filename = 'eval.tfrecords'  # address to save the TFRecords file\n",
    "# open the TFRecords file\n",
    "writer = tf.python_io.TFRecordWriter(train_filename)\n",
    "for i in range(len(eval_memes)):\n",
    "    \n",
    "    with tf.gfile.FastGFile(eval_memes[i], \"r\") as f:\n",
    "        encoded_image = f.read()\n",
    "    \n",
    "    try:\n",
    "        decoder.decode_jpeg(encoded_image)\n",
    "    except (tf.errors.InvalidArgumentError, AssertionError):\n",
    "        print(\"Skipping file with invalid JPEG data: %s\" % image.filename)\n",
    "        break\n",
    "\n",
    "    if not i % 20000:\n",
    "        print 'Train data: {}/{}'.format(i, len(eval_memes))\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    context = tf.train.Features(feature={\n",
    "          \"train/meme\": _bytes_feature(encoded_image), \n",
    "      })\n",
    "    feature_lists = tf.train.FeatureLists(feature_list={\n",
    "          \"train/captions\": _int64_feature_list(eval_tokens[i]),\n",
    "           #\"train/labels\": _int64_feature_list(token_labels_eval[i])\n",
    "      })\n",
    "    sequence_example = tf.train.SequenceExample(\n",
    "          context=context, feature_lists=feature_lists)\n",
    "    \n",
    "    writer.write(sequence_example.SerializeToString())\n",
    "    \n",
    "writer.close()\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/alpv95/MemeProject/im2txt/memes/crazy-girlfriend-praying-mantis.jpg\n",
      "other women post happy birthday wishes on fb wall flame on\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(memes_shuffled[150003])\n",
    "print(captions_shuffled[150003])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
